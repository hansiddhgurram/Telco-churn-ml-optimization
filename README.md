# üìä Telco Customer Churn Prediction  
### End-to-End Machine Learning Project (Baseline ‚Üí Optimized)

## üîç Project Overview
Customer churn is a critical problem for telecom companies, where retaining existing customers is often cheaper than acquiring new ones.  
This project builds an **end-to-end machine learning pipeline** to predict whether a customer will churn, while **systematically improving model performance** through preprocessing, feature engineering, model comparison, and hyperparameter tuning.

Unlike simple ML demos, this project emphasizes:
- **Baseline ‚Üí Improvement ‚Üí Optimization**
- **Clear reasoning behind each improvement**
- **Production-style code organization**

---

## üéØ Problem Statement
Predict whether a telecom customer will **churn (leave the service)** based on demographic, service usage, contract, and billing information.

- **Type:** Binary Classification  
- **Target Variable:** `Churn` (Yes / No ‚Üí 1 / 0)  
- **Primary Challenge:** Class imbalance and mixed feature types  

---

## üìÅ Project Structure
telco-churn-ml-optimization/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îú‚îÄ‚îÄ raw/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ telco_churn.csv # (not included in repo)
‚îÇ ‚îî‚îÄ‚îÄ processed/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îú‚îÄ‚îÄ 01_data_understanding.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 02_baseline_model.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 03_preprocessing_improvements.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 04_feature_engineering.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 05_model_comparison.ipynb
‚îÇ ‚îú‚îÄ‚îÄ 06_hyperparameter_tuning.ipynb
‚îÇ ‚îî‚îÄ‚îÄ 07_final_evaluation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ ‚îú‚îÄ‚îÄ train.py
‚îÇ ‚îî‚îÄ‚îÄ evaluate.py
‚îÇ
‚îú‚îÄ‚îÄ run_training.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md

---

## üìä Dataset
- **Name:** Telco Customer Churn  
- **Source:** Kaggle (IBM Analytics)  
- **Rows:** ~7,000  
- **Features:** Demographic, service usage, billing, and contract details  

üìå **Note:**  
The dataset is **not included** in this repository.  
Place it at:

---

## üß† Project Workflow

### 1Ô∏è‚É£ Data Understanding & EDA
- Explored churn distribution
- Identified class imbalance
- Detected data quality issues (`TotalCharges`)
- Analyzed churn vs tenure, contracts, and billing

üìò Notebook: `01_data_understanding.ipynb`

---

### 2Ô∏è‚É£ Baseline Model
- Logistic Regression
- Minimal preprocessing
- Naive categorical encoding

üìâ Result:
- Accuracy reasonable
- **Recall for churn very low**

üìò Notebook: `02_baseline_model.ipynb`

---

### 3Ô∏è‚É£ Preprocessing Improvements
- One-hot encoding for categorical features
- Feature scaling
- Pipelines using `ColumnTransformer`

üìà Result:
- Improved recall and F1-score

üìò Notebook: `03_preprocessing_improvements.ipynb`

---

### 4Ô∏è‚É£ Feature Engineering & Class Imbalance
New features created:
- `AvgMonthlySpend`
- `TenureGroup`
- `HighMonthlyCharge`
- `ServiceCount`

Also applied:
- `class_weight='balanced'`

üìà Result:
- Significant recall improvement

üìò Notebook: `04_feature_engineering.ipynb`

---

### 5Ô∏è‚É£ Model Comparison
Compared:
- Logistic Regression
- Random Forest

üìà Result:
- Random Forest outperformed the linear model
- Better capture of non-linear customer behavior

üìò Notebook: `05_model_comparison.ipynb`

---

### 6Ô∏è‚É£ Hyperparameter Tuning
- GridSearchCV
- Optimized for **F1-score**
- Cross-validation to reduce overfitting

üìà Result:
- Best generalization performance

üìò Notebook: `06_hyperparameter_tuning.ipynb`

---

### 7Ô∏è‚É£ Final Evaluation
- Final performance comparison
- Confusion matrix analysis
- Business-focused conclusions

üìò Notebook: `07_final_evaluation.ipynb`

---

## üß™ Final Model
**Tuned Random Forest Classifier**

Selected because:
- Highest F1-score
- Improved recall for churned customers
- Handles non-linear interactions
- Robust to mixed data types

---

## üìâ Evaluation Metrics
Due to class imbalance, **accuracy alone is misleading**.

Metrics used:
- Precision
- Recall
- F1-score
- Confusion Matrix

> In churn prediction, **false negatives are more costly than false positives**.

---

## üß© Modular Code (`src/`)
Core logic is refactored into reusable modules:

- `preprocessing.py` ‚Üí data cleaning & feature engineering  
- `train.py` ‚Üí model training pipelines  
- `evaluate.py` ‚Üí evaluation utilities  

This improves:
- Maintainability
- Reproducibility
- Production readiness

---

## ‚ñ∂Ô∏è How to Run the Project

### Option 1: Run Notebooks
```bash
jupyter notebook
Run notebooks in order (01 ‚Üí 07)
python run_training.py
pip install -r requirements.txt

Key Learnings

ML performance improves incrementally
Feature engineering often matters more than model choice
Proper preprocessing significantly boosts results
Confusion matrix gives deeper insight than accuracy
Modular code improves real-world usability

Future Improvements

SHAP-based model explainability
Threshold optimization using business cost
Model persistence (joblib)
Deployment via Streamlit or REST API

Author

Hansiddh G
Machine Learning Enthusiast | Aspiring ML Engineer